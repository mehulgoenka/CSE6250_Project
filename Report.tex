\documentclass[12pt]{article}
\usepackage{amsmath}    % For advanced math formatting
\usepackage{amssymb}    % For additional math symbols
\usepackage{graphicx}   % For including graphics
\usepackage{geometry}   % To manage margins
\geometry{a4paper, margin=1in}
\usepackage[utf8]{inputenc} % For Unicode encoding
\usepackage{hyperref}   % For hyperlinks
\usepackage{ragged2e}   % For text justification
\usepackage{enumitem}   % For refined bullet points
\usepackage{titlesec}   % For better section titles
\setlength{\parskip}{0.5em} % Set spacing between paragraphs
\setlength{\parindent}{0pt} % Disable paragraph indentation

\title{Reproducibility of GAT Experiments}
\author{}
\date{}

\begin{document}

\maketitle
\justifying

\section{Introduction}
Graph-structured data are prevalent in various domains such as social networks, citation networks, biological networks, and knowledge graphs. Traditional neural networks are not directly applicable to such data due to their irregular structure. To address this, Graph Neural Networks (GNNs) have been developed to extend deep learning techniques to graph data.

The Graph Attention Network (GAT) proposed by Veličković et al. (2018) introduces an attention mechanism to GNNs, allowing nodes to attend to their neighbors' features with different weights. This method leverages masked self-attention layers to address the shortcomings of prior spectral-based approaches like Graph Convolutional Networks (GCNs).

Key contributions of the GAT paper include:
\begin{itemize}[leftmargin=1.5em]
    \item \textbf{Attention Mechanism on Graphs:} Introducing a novel attention-based layer that allows for assigning different importances to different nodes in a neighborhood, enhancing the expressive capability of GNNs.
    \item \textbf{Efficiency and Parallelization:} The GAT model is computationally efficient and parallelizable, as it does not require costly matrix operations like eigendecomposition.
    \item \textbf{Inductive and Transductive Learning:} Demonstrating that GATs can be applied to both inductive tasks (where the model generalizes to unseen graphs) and transductive tasks (where the entire graph is known during training).
    \item \textbf{State-of-the-Art Performance:} Achieving or matching state-of-the-art results on benchmark datasets such as Cora, Citeseer, Pubmed (transductive learning), and a protein-protein interaction dataset (inductive learning).
\end{itemize}

In this report, we replicate the experiments conducted in the GAT paper for transductive learning tasks on citation networks. Specifically, we focus on the Cora, Citeseer, and Pubmed datasets. Our goal is to verify the reproducibility of the reported results and assess the effectiveness of the GAT model in classifying nodes in citation networks.

\section{Scope of Reproducibility}
This section defines the hypotheses from the original paper that we aim to validate and outlines the experiments we conducted to test these claims.

\subsection{Hypotheses}
The original GAT paper posits the following key claims:
\begin{enumerate}[leftmargin=1.5em]
    \item \textbf{Performance on Transductive Learning Tasks:}
    GATs achieve or exceed state-of-the-art performance on the Cora, Citeseer, and Pubmed datasets in node classification tasks. Reported classification accuracies are:
    \begin{align*}
        \text{Cora: } & 83.0\% \pm 0.7\% \\
        \text{Citeseer: } & 72.5\% \pm 0.7\% \\
        \text{Pubmed: } & 79.0\% \pm 0.3\%
    \end{align*}
    
    \item \textbf{Effectiveness of Multi-Head Attention:} Multi-head attention stabilizes the learning process and enhances model performance by aggregating diverse neighborhood representations.
    \item \textbf{Expressive Capacity of Attention Mechanism:} The attention mechanism enables the model to assign different importances to neighbors, improving classification accuracy compared to non-attentional approaches like GCNs.
\end{enumerate}

\subsection{Experiments}
To test these hypotheses, we replicate the following experiments from the original paper:
\begin{enumerate}[leftmargin=1.5em]
    \item \textbf{Dataset Selection:} Transductive learning tasks on the Cora, Citeseer, and Pubmed citation networks, where nodes represent documents, edges represent citations, and node features are bag-of-words representations.
    \item \textbf{Model Training:}
    \begin{itemize}[leftmargin=1.5em]
        \item Implementation of a two-layer GAT model with multi-head attention in the hidden layers.
        \item Hyperparameters:
        \begin{itemize}[leftmargin=1.5em]
            \item Learning rate: \(0.005\)
            \item Dropout: \(0.6\)
            \item Weight decay: \(5 \times 10^{-4}\)
            \item Attention heads: 8 in the hidden layer, 1 in the output layer
            \item Hidden layer dimension: 8 features per attention head
        \end{itemize}
    \end{itemize}
    \item \textbf{Evaluation Metrics:} Classification accuracy on the test set for each dataset. Comparison of results with the baseline GAT performance reported in the paper.
    \item \textbf{Reproducibility Challenges:} Analysis of factors affecting reproducibility, including data preprocessing, hyperparameter tuning, and computational resources.
\end{enumerate}

\section{Methodology}
\subsection{Model Description}
The GAT model enhances node representation learning by aggregating information from neighboring nodes using a dynamic attention mechanism.

\textbf{Architecture:}
\begin{itemize}[leftmargin=1.5em]
    \item \textbf{First Layer:} 8 attention heads, each producing an 8-dimensional output per node. Outputs are concatenated to form a 64-dimensional vector per node.
    \item \textbf{Second Layer:} A single attention head aggregates features to produce final logits, normalized with a softmax function for class prediction.
\end{itemize}

\textbf{Attention Mechanism:}
\begin{enumerate}[leftmargin=1.5em]
    \item Input Transformation: Node features \( \mathbf{h}_i \) are transformed using a learnable weight matrix \( \mathbf{W} \).
    \item Concatenation: Transformed features of nodes \( i \) and \( j \) are concatenated:
    \[
    \mathbf{e}_{ij} = [\mathbf{W}\mathbf{h}_i \| \mathbf{W}\mathbf{h}_j]
    \]
    \item Shared Attention Mechanism:
    \[
    e_{ij} = \text{LeakyReLU}\left(\mathbf{a}^\top \mathbf{e}_{ij}\right)
    \]
    \item Coefficients Normalization:
    \[
    \alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}_i} \exp(e_{ik})}
    \]
    \item Feature Aggregation:
    \[
    \mathbf{h}_i' = \sigma\left(\sum_{j \in \mathcal{N}_i} \alpha_{ij} \mathbf{W} \mathbf{h}_j\right)
    \]
    where \( \sigma \) is a non-linear activation function (ELU in hidden layers).
\end{enumerate}

\subsection{Dataset Description}
The experiments used the Cora, Citeseer, and Pubmed citation network datasets:
\begin{itemize}[leftmargin=1.5em]
    \item \textbf{Cora:} 2,708 nodes, 5,429 edges, 1,433 features, 7 classes.
    \item \textbf{Citeseer:} 3,327 nodes, 4,732 edges, 3,703 features, 6 classes.
    \item \textbf{Pubmed:} 19,717 nodes, 44,338 edges, 500 features, 3 classes.
\end{itemize}

\subsection{Data Preprocessing}
\begin{itemize}[leftmargin=1.5em]
    \item Row normalization of feature matrices.
    \item Symmetric normalization of adjacency matrices:
    \[
    \tilde{A} = D^{-\frac{1}{2}} A D^{-\frac{1}{2}}
    \]
    where \( A \) is the adjacency matrix, and \( D \) is the degree matrix.
\end{itemize}

\section{Computational Implementation}
\textbf{Framework:}
\begin{itemize}[leftmargin=1.5em]
    \item Python with PyTorch for model building.
    \item NumPy and SciPy for numerical and sparse matrix operations.
\end{itemize}

\textbf{Hardware:} MacBook Pro M3 Pro with 16GB unified memory.

\textbf{Hyperparameters:}
\begin{itemize}[leftmargin=1.5em]
    \item Learning Rate: \(0.005\)
    \item Dropout: \(0.6\)
    \item Weight Decay: \(5 \times 10^{-4}\)
    \item Attention Heads: 8 (hidden layer), 1 (output layer)
    \item Activation Function: LeakyReLU (\( \alpha = 0.2 \))
    \item Epochs: 1,000 with early stopping (patience = 100 epochs)
\end{itemize}

\end{document}
